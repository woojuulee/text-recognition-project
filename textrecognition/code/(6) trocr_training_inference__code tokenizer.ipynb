{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 실제 학습 코드 (7epoch)\n",
        "- model : vanila trocr\n",
        "- processor : vanila trocr\n",
        "- tokenizer : team-lucid/trocr-small-korean의 토크나이저 (RobertaTokenizerFast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WcEshvDDW24k"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_img_name</th>\n",
              "      <th>text</th>\n",
              "      <th>bbox</th>\n",
              "      <th>bbox 높이에 대한 너비 비</th>\n",
              "      <th>text_length</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>건강</td>\n",
              "      <td>[270.7996407778658, 181.6060866848643, 85.8899...</td>\n",
              "      <td>1.729167</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_31215_0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>기능식품</td>\n",
              "      <td>[231.2671741729349, 237.28789101405005, 163.89...</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>MADE</td>\n",
              "      <td>[2084.202921282695, 176.46746671166335, 243.71...</td>\n",
              "      <td>3.909091</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>IN</td>\n",
              "      <td>[2334.8476798738207, 183.39483441141803, 98.24...</td>\n",
              "      <td>1.677419</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_31215_3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>USA</td>\n",
              "      <td>[2444.426041669941, 185.91387721132884, 150.51...</td>\n",
              "      <td>2.366337</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50995</th>\n",
              "      <td>medicine_32162.jpg</td>\n",
              "      <td>plantarum,</td>\n",
              "      <td>[1025.4162565427919, 666.2598973080914, 286.08...</td>\n",
              "      <td>4.068966</td>\n",
              "      <td>10</td>\n",
              "      <td>medicine_32162_12.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50996</th>\n",
              "      <td>medicine_32162.jpg</td>\n",
              "      <td>paracasei,</td>\n",
              "      <td>[1676.3878095347584, 665.0476597792422, 259.41...</td>\n",
              "      <td>3.754386</td>\n",
              "      <td>10</td>\n",
              "      <td>medicine_32162_13.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50997</th>\n",
              "      <td>medicine_32162.jpg</td>\n",
              "      <td>Streptococ-</td>\n",
              "      <td>[1949.1412535258057, 665.0476597792422, 311.54...</td>\n",
              "      <td>5.586957</td>\n",
              "      <td>11</td>\n",
              "      <td>medicine_32162_14.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50998</th>\n",
              "      <td>medicine_32162.jpg</td>\n",
              "      <td>cus</td>\n",
              "      <td>[134.4216728387039, 747.479811740981, 94.55452...</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_32162_15.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50999</th>\n",
              "      <td>medicine_32162.jpg</td>\n",
              "      <td>thermophilus,</td>\n",
              "      <td>[249.58423807936836, 736.5696739813391, 372.15...</td>\n",
              "      <td>5.385965</td>\n",
              "      <td>13</td>\n",
              "      <td>medicine_32162_16.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        original_img_name           text  \\\n",
              "0      medicine_31215.jpg             건강   \n",
              "1      medicine_31215.jpg           기능식품   \n",
              "2      medicine_31215.jpg           MADE   \n",
              "3      medicine_31215.jpg             IN   \n",
              "4      medicine_31215.jpg            USA   \n",
              "...                   ...            ...   \n",
              "50995  medicine_32162.jpg     plantarum,   \n",
              "50996  medicine_32162.jpg     paracasei,   \n",
              "50997  medicine_32162.jpg    Streptococ-   \n",
              "50998  medicine_32162.jpg            cus   \n",
              "50999  medicine_32162.jpg  thermophilus,   \n",
              "\n",
              "                                                    bbox  bbox 높이에 대한 너비 비  \\\n",
              "0      [270.7996407778658, 181.6060866848643, 85.8899...          1.729167   \n",
              "1      [231.2671741729349, 237.28789101405005, 163.89...          3.250000   \n",
              "2      [2084.202921282695, 176.46746671166335, 243.71...          3.909091   \n",
              "3      [2334.8476798738207, 183.39483441141803, 98.24...          1.677419   \n",
              "4      [2444.426041669941, 185.91387721132884, 150.51...          2.366337   \n",
              "...                                                  ...               ...   \n",
              "50995  [1025.4162565427919, 666.2598973080914, 286.08...          4.068966   \n",
              "50996  [1676.3878095347584, 665.0476597792422, 259.41...          3.754386   \n",
              "50997  [1949.1412535258057, 665.0476597792422, 311.54...          5.586957   \n",
              "50998  [134.4216728387039, 747.479811740981, 94.55452...          1.950000   \n",
              "50999  [249.58423807936836, 736.5696739813391, 372.15...          5.385965   \n",
              "\n",
              "       text_length              file_name  \n",
              "0                2   medicine_31215_0.jpg  \n",
              "1                4   medicine_31215_1.jpg  \n",
              "2                4   medicine_31215_2.jpg  \n",
              "3                2   medicine_31215_3.jpg  \n",
              "4                3   medicine_31215_4.jpg  \n",
              "...            ...                    ...  \n",
              "50995           10  medicine_32162_12.jpg  \n",
              "50996           10  medicine_32162_13.jpg  \n",
              "50997           11  medicine_32162_14.jpg  \n",
              "50998            3  medicine_32162_15.jpg  \n",
              "50999           13  medicine_32162_16.jpg  \n",
              "\n",
              "[51000 rows x 6 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('halfdata/cropped_image_half.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "e2aWmz_AW24q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "# we reset the indices to start from zero\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 토크나이저 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaTokenizerFast(name_or_path='team-lucid/trocr-small-korean', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fl_UQmfIW24r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, tokenizer,max_target_length=32): # max_target_length 낮출 필요 있음.max(df['text length']) 찍어보기\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text\n",
        "        file_name = self.df['file_name'][idx]\n",
        "        text = self.df['text'][idx]\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        # labels = self.processor.tokenizer(text,\n",
        "        #                                   padding=\"max_length\",\n",
        "        #                                   max_length=self.max_target_length).input_ids\n",
        "        labels = self.tokenizer(text, padding=\"max_length\", \n",
        "                                # stride=32,\n",
        "                                truncation=True,\n",
        "                                max_length=self.max_target_length).input_ids\n",
        "        \n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        # labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GhZeAkI2YUAe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 40800\n",
            "Number of validation examples: 10200\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrOCRProcessor, AutoTokenizer\n",
        "# model = VisionEncoderDecoderModel.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
        "train_dataset = IAMDataset(root_dir='halfdata/cropped_image_half/',\n",
        "                           df=train_df,\n",
        "                           tokenizer=tokenizer,\n",
        "                           processor=processor)\n",
        "eval_dataset = IAMDataset(root_dir='halfdata/cropped_image_half/',\n",
        "                           df=test_df,\n",
        "                           tokenizer=tokenizer,\n",
        "                           processor=processor)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xM4CKlaPamFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "encoding = train_dataset[0]\n",
        "for k,v in encoding.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rtsz8KYNaoV-"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAABbCAIAAACK80ItAAAu/UlEQVR4AdXc2ZJlS3GtYYQ2qAHUguCW938EzHgXMJmBeoGQDhy68835rxwVtTKrKLa6ffzC8Rg+fLhHzFgzV1bV5g++973v/eY3v/n1r3/N/+pXv/q/t/3yl7/8t3/7t9/+9relBJ999tmf/Mmf/NEf/dFXv/pV8Ve+8hX+Dw6DfOlLXwoQMFUFeSlB/nX2Q8xE+KfCiX/5y18+axePMKTgQ3z71kKW1etDCul8PPvU9CNL7d7M0t+WESwz/JfwIlj+4R/+Ie/ZpSMbv6UnWJDaNKNJnfjjicr94he/+D+3CUj//Oc/d0VcC3L6/emf/ilO7ecFs6mfDU6wmb6wvo0Y7ws+88Y7T37gpx9vt3CFAvZZ+/fg3YR//dd/9UpwG7pQLsR//Md/SP3xH/8xqreCF4DXgzlUbZoCJcObafinj/i/yGx42zQD3/B8yP/iYE+tN08TNq3DHx7yVPV62X5HVs4+S8jLwCX493//d7fBOxPJ3RH/9Kc/lfJi6CcF30silSlOdPiZAn5xzJ7fHMZ+7wO5fjg2fP5D/DdFPgf4++rjGyzTrvLTf+IMtnkyE7xug08/89QzMZ5XhcvBLJ1UPzJ0xc86vhRpNZAAcsZny/9f4s9xuP+LW9u0Hf4nTuKxdgPwPUre8vGTwgPeIxRY/uxnP/OTwv1I/aJ+9pkfE7KnbYJJx49T/IXyBntznuHb0ce3MNqbap8Oru9TyYf0w8/ZxpzUkCfNc/lEVsI+8w3RI99tUGDpTcDEvigg8X1x5e+q95ybZY0cKlinQP6Lb83cRkxrmfW5+eLM3yu52ZqqmSFbireRD02+hzWCqus2nHKuhXvgraDrfofEUdMEqeS7B2KBwukWNF/iT6kv4HIn2MBtti18oaY13mYTOHnjfY5DPgsTJPKZB+9ByjGBF0M9eiuIXQ7ed0zfLsvW/pwAYUcmlmIEgWdqHIFeOIIn//pWVfWkU1WpM0bLgLNoeduBi/PIYr6vR8DGXskIK5mslGkjVMg3/zhVWfZtrF7zAh+5s8Xi/VxPCp6lb0jLdM4Tg+OfONo5jFQW7WV1/S/m43tD0hjkehNIa+PsWO+PpZBnqVxiL+f7Zu8Ip2+azURQzJ+cT4yrRT7Lxaxh0rFccPYKH2LX4pgDz8LFKxxScC6TWuuTUKzdU7al00ZQnsLrwhDZ+PxiqYHpRB4o2FM+lR9/+rR6xW4r7xJ4E3hzCHx5RADWj1Z7mHpZHoEJivk4N/Cea8ozS3NV71F/nwXBNHuoxQQK+NfjjROt5b2Py1mGfMoUbzJPcHHKNNv1xOtruUC8qvCQKQhCTpp4yxHQ9uA6f8hp1/eGDq72fDxvKreBNe6pqF6ncGRx2bysYPHZ7IwRKmzo+d9ZeIqIT5GlUjuXkGynYBnhDBaP1jYntZI2fuJqN3zx1NrsSRZH7vC3DOQL4BNZ+WvkpC1bcD6+FFIeDbhe12sgG4QXFe7bQ79Y9jUiuQjzgTzk7DTBEf6bgqe+dTm7byoB8tMYIfwMzSHaPiZw/tTcUzzVVjWw8i1fB7ok+9ojb6QKcc55IpyaPfuRBT7PqliNBEA+Zsvxr+8NDJqJm6CrwFv6M2mvkMlh3kXvHKQpK39NeEd9iZ52lcKnFL4IvPe/Z/cppzbl9tK3s7MYPwMKlup5hwwXEMyKxx8npCVmsucyQqk8ZEH8VSmsNs6e98CC+YKptYv2nkKplQPDIZ+5Ow5IhDSLQUIgy/oacdLirKQpI5Q645DTx1c+sHaqhnxK8GYXYKdw6lNr+dSi5eY5OWMuWBbyJE5/tE3+hKxqtafgUxVy1uRjno0QzqrxA5VkT5xTAWHZ6zY4iEQ9jwzC8HgE5jfMLsRZfLZMQXbzFb/mV4V2EqKFbLj/TEC/FnZ06gTm4YJ8+z3HkGLK+RQWbE4BS+FivzDjf6Kv6smTveRuo1OXwNolLn92Gb8SSyXZqiopNTC+bwXXDwuffjUC3w94j9/S31r5O0xlfkz0p9SyTseSCSpxjkpYXeFZS1Jl840ixgmJNo8QJ1k4Wj4+HyfaSQgpm7544y2bzpkiUhWv0JKlcI6xSdAQeOeWmmXZTvKqv22yHSxOCNmUazfl1HhZzPgteZKBYjbxgTWVOpnR7op3DqFF2xSjXb9hxrYNdg95zeEp9jLAthO/ZLo3cAQl+SQSBWYtMQv+q3yCWhDMp7ymEUazlyGCTAm88iEJnocilSyPH2GNKncCDVBWScvpWA58imPSqZFlmgWB6U+hgC9IoWV+fScVZ/uFz6TOJwiP/LgNcokK1O8qiL0G+rVCgJMKr/1UaNUpUbSnoOXp8aONPOSkLT5pwLU2htRsqSf8JBg+WaAgfyKJ1KLTFLO2XJyIOPL8qVa2VOcmzpayPEWUW06kuGX+HB4yS8TypcPjfzfnObzcmLWo/PE7BSrr/a/ebehfObgHwIQVsKmcoFpL2cA4+YGlPuKflJ+YZzbN08uy7VyqOJxUQZyBtXgCLeETF0B4Fl5QbUiEfHgixfEh7FJ5OaVqWw4UvKbdpQ/ndq5FkJ0WhL/2iePsfHAsp1NTy8cfklt78H4i9OzdBpUQhmQCPzUgaB78k7dkSc+v2evUOAtGHvJ7BcpZh6JQrKkt3PCzsxd2nouY2fi1jRdT1gzhLdGAKPw+JEPCLVe7VGAEWcE4Wy77RLYMMTYyGyI20o1df520Ac7A86pEORx/jQTLBj7eDRpYZ/XGi6oNc7jjwMdsskoWyy7+SIB27u0jTKk0z77bZMeBcwaphTgCVmwv13W4P2E2gia2O8oeMPMBCMdPk6/vufE4dYlQXNUTfo4tNWXBk85SBk7knv16W5tTTGqNSvFrWiN+su10A1S+pcKz9vreQE6zjkkOQqvvwP29ZQhvjnxbuqXee/9ESOQkiz9kTUN2VW8y3+yO2QwF+U7HKUi1tZ63mHWzu9yevRJZoMA98GW5fxQ+KUehNc/u2/L4ZUoV/E1TW3l+nBt+uGaL0Mb5cgX0lzVtZv4Tvzd0OSXtpTnz9U28GC7gz14Ran19DurkRBjpCtwGy1L9BNFP5WnIqfB0eLVDznjghwKy+Pk3OafaZsA8Z7CUgjD/ko83/72tyzlHSEs4sh2RjWOPLI4gZXwcW+MZ8kxqcXuve/G5ETSpfJxzfjGQRShleT51MTOnpn7nv+mPX0HhQFUucfOYs4H5tRa3HJLI6Yk83g3tTUHTCHxQHJwJ6qSZwKuik7q0X35eNIRsnTQogIjRzpZP8UqQ40dIUxxe6u55qZ3gubQLp9PYAn9e8pOf/ERgZoL+AM0VUcuLFfZrM4J9OURLtf1LH1mmCijVC0OtmBrvZHinhANv4DZbDKScNTmkJ4cAqZaOOBHZCHXnZfs6j6PcVegfq5q/qfC73yY0qn/DzFOzx87BtP5DGFkKYlmyUoLmadr0r38la22yhrM0gT0oEGtZjSWwA6r+yT+lLBXOC55M+RPSUpVUPqQlhBkJWMBHaGbehI7J4+TdAyfSfwHQ1oAM2EYCxfZrd/4rgSkU6Gv7WmDW9JrgZbxaz5fiIQqZwBHnBUNODjDOyhcYSdYkTGxT/vsGmxK7H0AP2HbM3x8MGtIWIN0GOji682J7bBcuRMN0CFrIojHBG7dBfRLOQjESIwdU/HEjinCLX0G1LZ98NISTXAycyKp2auMLME3lgJyCCR2Tx98fof7Lv/wLxJ5T4y0xIaT6lHQhuvFiODUEOsj0QyI7/YwUMhOwTbhloC20CyJsNEGpvGWFWp9mAEtP2j9XZv5Thn/+53/me4om8RoweZcD2H/zQs3wCcoyIrJoSoxhL+tiAATgJn/3GyYoXiS7JWEmy3MDmz4JHsK2t/gQqcU35T23wqGRzXBWidN5AlVhSnUcPvHM2bkK7oHjcwTXQ7g/lJiWKf/Zn/3Z/SgvR3OyltPsEO2dKXSCuvjMJaIpq7Z4W9BCDKQQOP0nZrR8G+GZFjXtZtvLP/3TP3kluOU26NLjEK+FILIuYpuV7YMRzeRxNLLBsmJdbEeq8Tbb4x0ozbCn6yoFOgiiJBgC07X6PEQqcJu/6ZeDLz4DtefyI3GaulRy7sGETs3+HZYb4GeqT09n97WvfQ2zR47WGKT+/M//nN9UCHbXBkkxjXoHdL2IdzL5NmuSpqJzqtlFhBNE3vBts40k0iTEpSACHY3hTeBmuwoC+4LE9xSVMMtkbYFB6k7BI+MhvHeGoEPDpwMUmNCuFTZS/vE7hTQDITFBnznFbh8tvWsvJcjf2OOh6qGwmVLIY75piZyc4vBKzix9KUigac3G3AOvBE9O4NTEaPbpie42tGdL9vWvfz0Fo7KYCKRC6mLXzCE0j9hI9yE9fhgTOeexVI4DZNuyWFUdz03BUzDSYl1sxIW2lx/+8IfidmSMRMzp3RZfTEG5oJ8C4nbkxuDXTi0QxwHaI1w5JHwjNeHjbVmD5BSguqF8/aLyzo5/0xqXeocSx5JyLd/09ZXCf5MQ3gaSCrE3D545sj5MttrA/UC1f5OwnZEj69MAlM2okc0gPkmU6djOlmLmUXUjzSmQFRBnRGgSaSOWmWWggAFPX4pyHbvZvir+4z/+Y98S2hcacbWGN55d4LcUG4NnKTcPr8rAzPlIKaHmuCDUXBdqDdCozXbdhoSQqPDmM5lbKd6ei3XFF+cFxZaqJlVAltWy+HN7giYhJWDJ2qE5bdIOxcwMtuDImBjNeBBjQ/pFS0kn2A/Oziuw7bd3tRV2rEDMBqAp27Hqws49Gu86lNue8Hv2x/wURIm7fPQ9Kj/sug2+LkjBWcz29Y1vfKN25D1O996+bIRRUAK3WRtEIwhBoyBAyHqmaoGJm6TgeoFYkGBaQhnR2si6FnZ+bfq+Jddu7hdUWvH5cD5EQGRLIP5pZiqbl4oz8MTTJygwBrIsE9DZFhAYhNm5Lfi54AtEW6vEgQrQamfLnSkp23T6shAlFO7Hcf1fWQgstcZRqNwt5Bs7qaYq7mqKGUG+UZUQaYA6YsoS/4d/+Icf/ehHboNY974IS3WP7UisqYFda7uwNXO6B+3OYNtXcyp0FZgWsmSZwAwQRkqtKnyg4PFrlcliCHS9End7oIMoS0isPUIWuVgJe8lc/1uWf8LHKTWRBfHPQjEz8WQFlvbDG9WuIKZVG+gSGJXvUKqVpcNnYgcxazDlEIQOxK4rke0ELFnnoDVl5IIUZM9A1jJ/l77LWtL0efNt0Q+Ifjf2yLuFsmTNo9xInqttugFug28PLoStRSDeGAJVyGSZLSj3khAgKMenA+SbuUKeXZvRrJ3ztKLy5Nwmk+khdetfF5ApwZwXpFNq+BMHPiu1peBExGcXWTvkp3+Nfm8MLobbnlhgcojzsnnnxVtOPM7E4RBmgwpptkSAiDVVTnynEYEsghRDZgX4bPO3zAOZcsxEKDhh3xLcBt5RVwsXGEYXrcVK7MUN+Iu/+AsXwu565+mbrOHrwivvYTUhjoBPiixzpVxoiMKG1+LxG2ZT8qAY12vlvgqEFCuomWAxPjAfeCev+TbZGaywKkvMwFNqJSdYPAV7MKcDEti5CXkcSzHvRSpoL3CnzzMp+mvas7FkxTg9qj6gFO66C3QgfU1B9oGjUwueUYY7QPw2uI0smBSk6+USMD8XugpEdJQV6HiNdb94PDz3INs7r72gkS1WWFUdDcMgjD4Pd8Mo9FPG2AjAm/jlx58+tQEQUQxLO1csAJrv2u5thqtTJXnI8LL8Um9mR/t4QGQ618G8vHUFdqXWUI7V2LylUfs8QVhbpXCdx/F2SfOSfjG1NOOLq7Wkr9Cy7Qt8PBRp53z7wIhv+cfbK8kE+TNoyeOrdaX6wyWXDEKZZhuxC1lLM7gKf/VXf/Wtb33LbfAUTWKb56gm7w7Vmk4D06TsM4OgHdOagvfKbkMlCFLXg2cWGciyvQm03OF2NC/Exw8LHCVVSRXPQ0jxtTjxyCsZ5xRMdghOtNTasKkY3BnxDoX1CC/2y8/RRFpONnD6guK77roZWvBA52sjYsrKETwzS0+L7bhugYerF9+uz0BM0LPx61+PXE0Esinfz+56eD7Hf/mXf/k3f/M3f/3Xf+2FZ3eYWk/WkJszsCxakwsQ7oO53nOugp84PdY4quD8Y281qNgcBjKu5d3oEqoMImB1TeKMpYCbZsw4+Til3iSc5FPcoVvmJ9KQOjabgJWFIPNMIZA9iZd6AiPTafs4DoQUxCECEVwCfmf1WuFNZDNQ6zYIruf/8l31mv7uK4vsyXklfPvb33YVxF30drFNVeJ56QjM6o5pYOKWxhZDvCpYW6vEDGlet4GtgUpzsHA1Uvrp0f2IKQtZHBkyKallTyZwywj5wBRSm2agiQWzicM7Pn5SQNYwTyXtRQo+kZbzFWKyTiCRDrRz8AZW7rhaKqkqpvhJv+xFenlmCu83yy+9IfzhY+3mlXsr+NHgNnQVvJYmrjWznJrCxQUhZnYUyAIGdKUKxFJpJnV9iSiXeihEDVFT2jZOo0DQLJ8MWEsBQ2NJWQrKqiobYqkRpiyLnwfGxLm0Xv5Dj7JS+DxzZCs5FaQsQ5Rb8pYKCyIU855NqZh84/HMsgl5HxWvd78WOiIpX++BroVlyPTv0qs2wzEAEN8N8PukL4+MGnPC9kIBTRbzm9/85ne/+123AU6hvSinzyDilKVUhSwLZEBvAm+FYgeewUN0rIR//EIlYURrDYy1HsBTRdYyQxbkC8SvjVRgmlsCxafCUyrCasccEh++YKnXoFQK7aglJJAnwloK7DTDd3y9b/uK0MsAU+AZAHvMt8C7n0SWzZPfSVoq9BnbX6xo5AsBD3ctpJB9bfQXbH46aC21wVJ7Egeue6l2FM6nMA79J4WW18tHjtVPmc3zNmk4li5EQWTB1CFZU7726yrI4ogL6l6c7ETGgehy4g+t+yNbFnJyRl5t2Q9xOgFZOk9jWPqE8D17TLGHlDLQKQEtGzj/NABOOKa3i6vgreD1IPbppAb3qyZcAPnOd77j3eBC7OXXYJM9uxjbSBtgQZyTKYXMnjgtH68BBdlIpuyjAHc/eFMOSfH0Cn8vI4g/BbH9ABv0tdTwChEWFI/wFFjOMMWJP4F2x4DG4LeM7Fn6yHqBO4FdBWCCJ/mcau0C60ihLwoE3ST3zPFKpQ/xYncJXIV+n5RVQkpfsWAtJluWP21ZQbFsM+SHlwW++97Qw6jA9owldknheew+BBCx7OTEH7KOGL+WvJi1sQ9Vhav9OGHZNPPAcyneDK8JkdNBizlfsI+yp2h+z8+BiB2RJY8mYOmccYi+HRpyT50HOgRvBQ+7V0XfHlwCv0/62eFaVFW5uCXNs4VsXT7iNxvO+AsqxLluAzMWE0jYW681e+47hFQqCpJomrNHip/uX+tAAt8Ued0LefPcpc+Ojr0wiQJIe3zdwpYZmlSyHpKH56n7wth3PSkH0pl4cvgQJfhZtfxpzRmCTNZV4OFOGE6nrwvi/szRLxF+QBjVABWKtagXZO3acxzecjHOkHOGcKmCLXGub5FR9cuSEONJIfhwWBp6lWlZZpYfMoRSL9zHkixNqbylmN/NOwW3GQE8XzBErXhWFnga0HKFyMV8T9cegUblPTB2/8HxT3sr9Od3FBwIjslZ98PYSpRDqD3ZNAXKUxZYdjPwvSRoarG/QQB2G/ZBpa/dk7glnbygmBSkjQj0GrIAzloWP25D6L276xKM0eg2j2APdYocbX6pdOcnNWapzqKzK/V6G/ATrDBk7bYUnIa8pV61g3jqS52BPTr63QZ8+4V4MfRDwee4P//xVkf2zJB9gj3F3hZK8Pu4N2r6nYCsZbEx1NaOuGVXSpaaZy/oU1FVSFUfug2y53bqBRxOk4XvypYFZtcf8ovuqoert3H3wwzBJuFGIUQUFQFeA6As0PQ8MGRdK5GNH0F5IumUGkIHno+QvngzlO1YlXdSHpUHaWA4aySeMnMbhqcfR1XMJqdg+x4VD/GHCp0AsirPTArS+5xsWzODkfCZGIjMxBDLbpiz8p2AMlMrJiVb7J6xtmZHPQ5ZQe8PaqcsZUt8dqZaUhCMUHDSJiW4fqfQqUr9et4QJh2CA5/EWQ98wsvyNFnLM8DfcuTfGVRVL54lYk6x0+SJ9GCkmLj3sIB5EsxnvSWycxTj9GLY9qvl7dof6aNJBaoCWuoL4WkSEUs1lZiJWxbIjtAAOJTdp8TbiBiohC84yxM5PZGW0Z6WJ/MpbsL4aguuz3obwzYKk4MwwS5BQWCN5xWKyWV1fVk934anmT59Wbt6LdalWGC2TpkX++wyz74L4Xl7K/A+jsh09lARpJyDBwOX9YAFlj6j7plUIBpxRwTB6Q55WwAbjO+IKmk23nJHR6qXE/J93tfFanIjsToqySaC3+Q80PK0J6QqhEpOZvGb+Ls3/9Ims1VyZvVqaqtiBNuozWsvy566QjDD8y35J+bvXNYRbUElBjPVPqCm7SH5p4Ues2ffbfAgXQ5elZLezI47EQ/eToEehmfTY3AVfKcL0cL8Yr4nJ8j2+NvUC/zuI0vt3C+CEqY1Qanmh5hhY1R1blah5WtPZ6CgHRWcfcNPP86Cx21I0T5N1nCOZpWNrqZg+BnIrve5h5Pzn4zTT1w71jzGFsAhroX/nsKf8f393/893JLZkTvBXBTfBB10jwFBlR/bbgNL35AC23cbPBtLVTxxhUTcMLcqWQhzvXicxhA8WbMBr6HvBwa5S9/9J9EQZjCW2pPI05IOfmpSHcWl/qJfO5ynwo8s3/26Qs6pOSByDcqHSDEqkLvdG66ufIYsqLFAwbn8yEAfStXyzBoJ6MHwcN5j80rwb039ExJ/QhDZjuyi+T3ynrGHjQ909O6Hd4AHn6BHK8bkxUCeVLEq7xiaLLAsGtveaz2vilniVGWA7hOdUs6WFN9BpXbXPY5uBwhcvBaCyCFpvklbybILrhejdGsxFTM5CzFzjoauDbxDbDnfHLaRDqmsruKC+RDlQz4lqN2YLZuQoDl9RfDPjn/84x97K7gKfcJKyVp65D31zh3ieffjoBcDTXzblLJThqm2J+TaMR17foHIShoD09KEEHaOWozGpNKvVoqUXkAma4wxL6FDStyQp6dgSWQdC+7Sd2N8JLsW7/68YRKN5X1oJjtkpUxv6ImaQMyz5GrfWCGl5vHHBE7qU4InvmWmnTl9Xv/u7/7uBz/4gdvQ90S/CzTeTtZV8Ie+PUt7sfTXAX517A+A4cCdqTktGZHOwc8IHwy+AymVh0i5UpZtcDsypHin0RLNIddrHlJ5A/OqZE+FyQrgCGXXYkgdtzwLi0c4l9dMFHnT2I/n7X3LTIMHjG1QiD17rxLKpAqUMzo4jnWgABieDo/JA4ecQVmI2hOfLFAXRgEH7r9B+Nu//Vv/qZr/ZNHShL3qXQsvDIL9o0KvAVWl3BV/EQCxwYZ0JwQMv2DdLR2IvVOjCVco6FVh6WK5EOZxSo1nSSdTLiUW9L4xRnyIWsossG3SMaeS7scmKcAcQiFD7kjzI4RXIsXwZSFiQeUaYV7/aYoEyDpobFSHhSSQ4tUjQ/LJwUudy5A783jw6/1Ei/M7/XZlPArM5M7UVfADwk8Hf4QMdKGdo2nhlk7fk/MO8FYQ6GJHCGKPIealdZ9R5zLfSLp4VN0GcWTb19oSWS9dGGVmziksTkqtwnkgJoSRytdCyjJCauJaW4rfNCmcpWJWdYITPKWiXbchqpzpmaVc8wlWM9CgFefjJwI5l4Gv/QrfTL0GIcY4q8xw/xz/ue+M/rsUzx7iGXsqpXyOvSTcA//E1N8Oez10G7wD9vwE7Y6yKnG2dk3iKHr2ls6nk3FFBB4/kfsaPO5BtfdBvveOoZ/tYNelQ6t1hTTRwkOa5FM8nZOmaTq1OzcYrSyP+fhJ0cIEbPWC9pyQFFpMKUY6BEGQ+vyJLH4djF8wwmtcR6DTwTGY5+2XSS8GgePz4F0FH2JfIZnYd4L+famUmwFR7pVg7D2tukAoiHdY4Tyc4Xvk6WvtciSyLKbx4N43T1toyc/WiOxaA/tJXbtOG8gUYgqQb+D3cB3a69qkKC8QX9faHtTYpKMUx7BzAwF9b6JlqcxWX+viZ+lOHRN+ghDWfOEhn+INhuakFFIwlR8Nfp90FSz7AYHTHzFB+q7gXxD5GeF9YH4H2jwUBDgCVvcezNMk8aV66siQDoqmYG8FOOuzVCNSgWdAQTbrqctahlMTq7IRCJM9A8uP2NqNoxZomVR4SH7Mltd3KGaRVWkmx2e3ggoEZu1MK+GlqnKy0QKLpQrO1LqcqTN+qlpKi4YRuAq+xPUO0BHOu8pAZkhvBf+4tH8+5K3Qs9xetDjnPOPaNWReu8wT0sKPJ8fiqXvHIFe7mV9L4UxNvGOk1lQj6EK2LjbYzzWCytdlsaonUw6pl2AlTzSEaOEbuEbvfYvspvOoto0h9iEgEail/WQT2gRrnPS6jjlCM215Bh9KaZqsh+GReyX0YLwVnKDBOkflfl9wFfzzId8Y+gGxCYmcvYrreM4c//Sek3b6ek7wPipiCnCBqeBEngwBXiMpN8BJKm9a81iKlfc4k4Kcs0klwvcgyp5eKlthk1hupBfK9b/hy4a8dxtMydpY85meFqrATD4c/Z0esK7zZ4NASGAtF9/we5/OiXwk6EGqNZgjY54Qvh9ehhFLOSlL19cl8APC75D9FDd5J9Ik5/ANE173kB6AuFto2TPTyFGQ9Yff7gGCuHvZPDVSVSFC7QqAytmdv+63A5dysMot24hJgFlxsk34pn+hv/c+xqyQX9Vkyw4XPP7Ew1bNYVc+cz4B7dzQ3ldiRx9BAS1Mp3Nv6t138rNrR9mG1UoV88V0tu0hqljPr0bnDj1pTDiOC6GcmhdDd9fShEbyM2L3AFN3x12tctYMxdSY+GwkHi4gAvG0fCPBpA8RayfQ0cB9a9GFuBL8dnf2AiYFZJZM0H67DetLFsJD+KatRN9ofPiygkA+so5Ag0GMxFPTlAhCNJ4l8t7vFIpZQk3ZKQ9MouJb5A1XuYRAD73HX3nZLeljVnjudoRScWoZMlwX+7T07L0bXGIIa5OVnIXhT4IjCFIWmM0h8NSIKzGhRyUFcUrdtnoBp3wGcOPRcXfFxGWVADvbCntUEJeYMjBZeARVasUtC/jX4JiC0Qoir+QEH/9wI0VzCNQ3lj2bnh9OQvbJTrmzE1r814RS50DizFnLVnIGZmj5Qrz+F8jDDVzgY+r7nZ8RbcFpnt3F0T4Cxklc7MF0AgQ9+Druc+ZjBzyvQjFfYDZGp5tUoSWQoPLenWKgJdyZe0lXfsu8d6cNVjnPWgowb+A9V+v3oHtR1WoFxdc7ZDUUG1HO3Lxn0+OBZ5Ou6pJ5eeco796EiCGjKQyfwpBxLq2X3QJnMSceJx1ThQvwHTrzwdJ63WOqQlhTwdMy2sjXKPfXFCcggDsTpp3H1usBbhkuJjhfMM2WRnqc48tXBzg177O68JTR4JTzLcVsggV1fALPZSV36VXbWbVc7TiPfzSnn5k6yoZwQwvULCWo8uyHlq7U5kYoXmqFENnwU6c4WpyQ4vBGKqbvZHGcoOcxEB+tFDADMnFB/kl/IJpUhXHEuvhG1bFojayppQ83vwEqlC2onI8Ad4Yuq0duQiCpEO8DBrdkpeanI9ic4kY9g7V+Sm07AudzCsbkH3+jbbKugjnsrfNVYNm4AhwxIT7pU1dsDl5VzUIiQ0q1k3Pi4qqipfPkESggMLGseRyrZdM6OAgcYgsGxq8kKZMI1L62mMOrujrdveh7Tv1aS5lOUrKWDD+Qgrh2BdM0HpwZzFH3i/H4snQsaWrHXAsKlReotdRoVcvWK8JZFd4u+FK8dtVeA72cCfw6MutuA4/UKIo1NjREUCpFJaelGx9uuSzl4o0C6eAmFUcVhLXkIaffKeDQtOymmtaSh3dlVdktMDXM+M0pZjgsJL+nZanQMk2XwF+C9P/h2INXnrIAmX4/mzoiS8p8wd3n3V7wIZjd4wo9eF2owWWJe9+IZcV1maaluJl5y9OkygZGCJmUwO74m/twCT6+NzQ9T6XbIGi4tHhIinXKh5eiGNlSjEywkjwQwiP0wCqJnwiFRFbecoVoDNh46eQHIii3PAO9WLggkQKeAn5dcCyZwJ94+vNvf1fugbltnuLIgjoCmXlCgBk1AZCJdaQpBiL7pHUbdGFSONHcDwHwBh5fNW6Zx+MvPj3xp+WQAmoIvCXzlF8Pef2kYN0Ad1/crO3NiF6Ssn5nkxXwQOfii49Z256lBjXDEUB4u9pxjKCFQrQQcb0gjCyCVLNKsTh6VeIQ0RAgSsxW3CMBenK91eBqG4yvHBhSo1rUUW3jqfIB9Vbwz2f8uzpd/ImWRmgOxCEgQHThxSmTlSJYi3x9KQukulIEHQ5z28RSiWvKzECHJmUl9pUIfGpS9OvFW0qV5ddai3bEa8STZeZRwsTxr9mUWSN1EJY9SNM0Ma8GyKPBa4kv1dCyrE7AmqHh6CFgdb3639s4O7YlBOaskY2B9nqTEEfTYGjpQxQ6RMMobxhLZATLqaE1CfGNBDSMJc+IqHKIXgnM35GKiXjwBLXmPblau5ekVFlSQJOFME0h2RlDtixA7ocCcUggQYFhmAGSrfamXD9QYvKLa9c2iylbUiPOz5yPVO+nzXx9gjsOiqnIaaAsFbFNqhRQNDoCcqfPU5D1SKRUmV5hImhqG4unwNDEUlnInbnuJYVa0EQTo8nmydbLAdW6B4BgNiAFk9hkj5AChEg6PCYRoKCsjsQhvKr+FtQfPHsluA3+0tx7qP9iGgHZABTQtGY0q6XWeALi6RfPF6y7pXIDUzY/3JLBSVFoJHGgJXIid4d3XaTgpzUATTMTZ9udgGDv17ZQi+tz89qINlmDKoB4zDWzrIETARJC6xLwOqFBqiKemsAE9SLeQGoZ0GZ6KrxCj1OVVB3xmRgT2IWIA7dPfYmnT4H4uitJhEe25COHdI5AVX7EeHX743l/Xe7HhFgXf5zlxwFBhKY1gI5JpQYvVS9gQSOdM4RDmqGnC3SYQCKQNGPanRcexJya8ner6yeOwLIx2sXEC3gzSzkTH57+DYA9WvoLXuUE6QuIsHdv0dRBFSMZ11lYskSdgrEU832GpHCQEYBaGp2IE3T1MCFwBqzrrXf9Ub9te+XSbGiFhutJQ+olUMgELCmDdWS8KoJw4tQ6fU2BFJq/rSHIOghqApYgBNkWHJNXQn9Ts4vubz2INIwSMU9BayVqicjarC7GSB8YbYFU7dpLMzQzb4CuGtl6tS+4KpptXEpT/GiW7QInpiCrSzFlav3LIFeclGmllLAE418f+ibjs8nptEHbOU8rcOdlqVCKtMZMoefa6aDdTR9fTonbGzOfmaRsibdkqryTtXDoPJ2ZFudgHQQdfeM4IJPgEHEbum2yTKoLgWlJGS1Bse50XAX3wMtALQ5Qleu1P+TGJEJBgGDCFIBiprsUS1yW4Tc2X6osvGmj9TFA0FRr+oZpMKAgEXO2l8rDk53462WvBL8W+cHnkL3nOnnlDH92vRtC87r2dNt27I6+GnML2hLccLwlvI0p7whsiUmlXFYtJhwH7ug7faCtlu0a4bdnnlnWdF7ApHRsDAGR9EnR10hhvgF4HFY539JtcIEMk6ZNecBOrcBI4+NQ1rGdujGyExc00jX0y9iGyWTpjCOA0+lK1YW+kejzCDyOTSns3IACVY0ka56kIC15Zultt3tAxEiNbbNKbJCHZNc21JDTNVEzEbJJYzH17RbBEFLXLu99knBkYjgmGitFFp91VwQQqcp1VMjD1ZpSa2rKEZgUfiUJQgSsFL4gawxV+PBb4HoZtkNL+tco97HqK4DUhcKduZBAM5iZOQE/79D6MBGv0MCqrq3ezBrVVMeGLFASvl4CIKPQXmha4utIEE7fLryTBF60Dlag0HgCZL6ZVcHF+GladgI7B/+ZiVeCu07fDYvmDaFKOzRjXDu5H9x1G0S8xkjkHIEyd0dXVC0tcSxxKmu3jQIX4FxHeL8nGtfGhiAAmfb0/SsBh9jLWWsxZqcjIM40YkTUWqJlEIEUT607Lk4c7hw9cvM4R9mesSpGCuIgkHkIfrvzGRI3iXJXgQlMy5vN3g2so15KSEGI0xHzUk2FUC84gzNbICLFBsLFEC3UJtgMflLoAufN7NGYGdlUmjID60LWiam1WQaHqJIybWY2HCDTEY24fxhma3DWyRvj8fcU2oA0pkVaLLDEECuwTLQNV6/KiFJ64EgxYIhaATNZx4FTo8otPblriPvrMZoWZrXUmoUnohCToRXk0dLPU4anr1ALy2p55rwwGXE0nAgmgXRknsd9Ga5vuBAc83tUqpTU0VLhrXQ5OKs1n5UVl3qBH/O0JG4qMbIB7hkf90NTeCdsEllLDxhHCj+FvKXTq5wXd4cwr6O8/0kYpuOFaGqDtgkna5ldvysmAcWoRwhqS1RCQELGAkawVM5bAu8DeXzJaklTcHIaHd+uOusGQrsu/P0jw5SuLfMk8HWn41CqxSzoiHEK+ugYRqBjTwuzDapqTkiBqvaFwPwLP/N0D/gMgmNaH82amkQhD1RFVozGp9YwkWUZBHk4RDYfKDZSJTyQsoC3i6alX0nfLiOnbL9Miw4KTRVzmGLHqLY7DRE7cOZ3JW9oWTs1f30ff4BIC9RwYurKLNth4/LReNaUxVKMYnh7U8vE1NqeZRuoVqxLfdsPmh/SzLMxqFSFxJHXotEttdtOmgT/ulP3QehCn2dwHmfnqPw07wZn1CfG2ZFlHYJa4xneUjkdwyArByrs4oo3WF2m35KH8MYo4LevsiE6CmqEYBIDWALNUAxnmG6qT7yUkWTbqVQ3wGwKGVxfz97Z+g9M7LRz7uOknF133P/YiZbkqKh0mnDS7ZBQWlfBy9usQkhbtWR33+vQ6ajlja4Wjqa2s4BoJ5sRgfQwICY2qCmRFd7Cj3cpJoPzmFJNSC1Quz7HUjjKKQNZUh2ZWssuDUQvp0NEYEmT4TRtvQwvpYoUxJAV7p9h1rEx8mivg7YjdWaLNZ2IsQ3ATHWP//gbIoJosoLMYAgm73hJIZgto+BAjC0ws7vrG4O3QputSglBra/Hc5oaKA/sRFBpxZYCQhpasFoIGisFJ8LwgZjMEo7Azv2U9TxK2ZVZkeHxI/AI81LiDZk4X7vKa+SpNz8+XIlaCLN0RszZrW/KyAyHhyAgE6QmsASa2UfNmYqb7ay9Bd5diAjABWeskHW89HeSyGLPu5c/zj344+c7BWcFzCNvNlMxP5Fp2poPmEvAM/sFhqtVxZO6bgOqqDUtcT4QrkwzM7GlZLP2Bo8QeNLERNAyBMs+YQIWEk2MBqR2Jx9utYJaVGX4p75GlZqas/PkUrMRKXEi7YtC56IkwnrVgh/TNCavKe9YnTiwi6V1yk9DUhh+pqZPQWwAnuGQ3b4sN2StuxARMJVALHmj0kEzjJjJit1aLwaBFANquqnE7LoNHZwEuRu8nCWrhiKk3gJghgAUCxIpyxekoFxW3MRSluxmXWpSxUBLx31uLH63B5PVHS6wMamUK2//VSEkFVPK0oesJ+pnqotCUPk10PEw0NIPxOkQ+E6ZgrgXhvIUIPAmzBMRJCXImp8P5wsga9e+Ursme3lba0FEO68o2UqAfhzcU1wbMRVL0yXAoebimhyI0Jz2SCrlawL2/e9/H/u1Sb0GIUlQQUhoO1xQ4aV+25s6yKMJEFumvJI0I8fJhzyRVyXoO0HlJy6ejiAD2kuakV8y7w7Bxtu7FLKjH6cuZfukEkntSXPdlyVVxydP/G74cLXg25cs/ghwXyRJZVIv4XUzxG6DG8DqMjVLMZ/g9cb4r7IUp/a0HP4/E/gctGftzg07QUiz8dnnGOml9Hoq4vnPIfVmCU1Pcam2UJfaiYG7EJ40Pr9sCFo3A96B5E/xdfm9b0OiqxeEUNdmWcFSJ/l/LG637fyp6WZrwpbdkifmR5bpVxutE/hIyedI0SfLd7YUWiYFNLa3lMA7SYpJRa52cXglfMx05v8fCVeAHQMcHhQAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=175x91>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aIU5nzVKa8tN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lac\n"
          ]
        }
      ],
      "source": [
        "labels = encoding['labels']\n",
        "labels[labels == -100] = tokenizer.pad_token_id\n",
        "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "# labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "# label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qaf6EPb_a_Uc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRcQUmJbFVV"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "b_bQks7XbCvN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): DeiTModel(\n",
              "    (embeddings): DeiTEmbeddings(\n",
              "      (patch_embeddings): DeiTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): DeiTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DeiTLayer(\n",
              "          (attention): DeiTAttention(\n",
              "            (attention): DeiTSelfAttention(\n",
              "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): DeiTSelfOutput(\n",
              "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DeiTIntermediate(\n",
              "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DeiTOutput(\n",
              "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): DeiTPooler(\n",
              "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TrOCRForCausalLM(\n",
              "    (model): TrOCRDecoderWrapper(\n",
              "      (decoder): TrOCRDecoder(\n",
              "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
              "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
              "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x TrOCRDecoderLayer(\n",
              "            (self_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (activation_fn): ReLU()\n",
              "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained('model') #모델 pt가 있는 폴더경로를 알려주면 됨.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XYkvF3XxbHPj"
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "# model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "# model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "# model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdzekHB9be2l"
      },
      "outputs": [],
      "source": [
        "!pip install datasets jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "WzOUi5albaSk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wooju/anaconda3/envs/py39_trocr/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/cer/cer.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "B4HfDZSBbbpm"
      },
      "outputs": [],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # 빈문자열 제거 # 왜 아무도 안알려주냐고\n",
        "    pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
        "    label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1n9zXxFzcEiR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wooju/anaconda3/envs/py39_trocr/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0d7a5fcc54e4f8fb889e42de0d3d9d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 4: 0.38803601636809404\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c54d3c996c3940ddb637ecf21fb999d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CER: 0.12408556682878534\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56335aeb25274b97b0c574bbfc7b418a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 5: 0.2918694868942622\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbe54f9251e54e6fa0139f46316adc91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CER: 0.1103732389453185\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "730a530b976e4278a6f4f70359aa59c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 6: 0.22914171106580272\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "034e060479ac4143bcd59ba818aaac6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CER: 0.10557820137243855\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(4,7):  # loop over the dataset multiple times\n",
        "   # train\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dataloader):\n",
        "    # get the inputs\n",
        "    for k,v in batch.items():\n",
        "      batch[k] = v.to(device)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
        "\n",
        "   # evaluate\n",
        "  model.eval()\n",
        "  valid_cer = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "      # run batch generation\n",
        "      outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
        "      # compute metrics\n",
        "      cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "      valid_cer += cer\n",
        "\n",
        "  print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
        "\n",
        "\n",
        "  with open('trainresult.csv', 'a') as f:\n",
        "    data = [{'epoch': epoch, 'loss': train_loss/len(train_dataloader), 'Validation CER':  valid_cer / len(eval_dataloader) }]\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys()) # fieldnames=['Name', 'Height']\n",
        "    if epoch == 0:\n",
        "        writer.writeheader()  # 첫 번째 루프에서만 헤더 쓰기\n",
        "    writer.writerows(data)\n",
        "\n",
        "  model.save_pretrained(\"./model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOgt-3G4kzhe"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_img_name</th>\n",
              "      <th>text</th>\n",
              "      <th>bbox</th>\n",
              "      <th>bbox 높이에 대한 너비 비</th>\n",
              "      <th>text_length</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>60000</th>\n",
              "      <td>medicine_32275.jpg</td>\n",
              "      <td>및</td>\n",
              "      <td>[2063.549032545688, 721.9072080204544, 45.0854...</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_32275_32.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60001</th>\n",
              "      <td>medicine_32275.jpg</td>\n",
              "      <td>[원료명</td>\n",
              "      <td>[1878.9132963780062, 713.3194993614924, 163.16...</td>\n",
              "      <td>2.268657</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_32275_33.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60002</th>\n",
              "      <td>medicine_32275.jpg</td>\n",
              "      <td>옥수수전분],</td>\n",
              "      <td>[1976.9988783403421, 792.6044345922198, 266.49...</td>\n",
              "      <td>3.297653</td>\n",
              "      <td>7</td>\n",
              "      <td>medicine_32275_34.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60003</th>\n",
              "      <td>medicine_32275.jpg</td>\n",
              "      <td>DL-트레오닌),</td>\n",
              "      <td>[1542.9191950961197, 860.3840101462162, 326.33...</td>\n",
              "      <td>4.222222</td>\n",
              "      <td>9</td>\n",
              "      <td>medicine_32275_35.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60004</th>\n",
              "      <td>medicine_32275.jpg</td>\n",
              "      <td>말토덱스트린,</td>\n",
              "      <td>[1584.7842748085595, 937.6733880768736, 322.03...</td>\n",
              "      <td>4.347826</td>\n",
              "      <td>7</td>\n",
              "      <td>medicine_32275_36.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102002</th>\n",
              "      <td>medicine_32820.jpg</td>\n",
              "      <td>유통기한</td>\n",
              "      <td>[109.49328779974199, 3030.854831940932, 337.68...</td>\n",
              "      <td>2.941489</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_32820_97.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102003</th>\n",
              "      <td>medicine_32820.jpg</td>\n",
              "      <td>나트륨</td>\n",
              "      <td>[1288.428836456075, 906.5898020406504, 276.353...</td>\n",
              "      <td>2.428571</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_32820_98.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102004</th>\n",
              "      <td>medicine_32820.jpg</td>\n",
              "      <td>25㎍(1,000</td>\n",
              "      <td>[1039.4533106824476, 1015.1929759969929, 528.3...</td>\n",
              "      <td>4.055687</td>\n",
              "      <td>9</td>\n",
              "      <td>medicine_32820_99.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102005</th>\n",
              "      <td>medicine_32820.jpg</td>\n",
              "      <td>까지</td>\n",
              "      <td>[1454.0794263845135, 3021.2979179264426, 182.4...</td>\n",
              "      <td>1.565934</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_32820_100.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102006</th>\n",
              "      <td>medicine_32820.jpg</td>\n",
              "      <td>(</td>\n",
              "      <td>[265.68238149494323, 1255.7630390585678, 49.26...</td>\n",
              "      <td>0.423546</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_32820_101.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42007 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         original_img_name       text  \\\n",
              "60000   medicine_32275.jpg          및   \n",
              "60001   medicine_32275.jpg       [원료명   \n",
              "60002   medicine_32275.jpg    옥수수전분],   \n",
              "60003   medicine_32275.jpg  DL-트레오닌),   \n",
              "60004   medicine_32275.jpg    말토덱스트린,   \n",
              "...                    ...        ...   \n",
              "102002  medicine_32820.jpg       유통기한   \n",
              "102003  medicine_32820.jpg        나트륨   \n",
              "102004  medicine_32820.jpg  25㎍(1,000   \n",
              "102005  medicine_32820.jpg         까지   \n",
              "102006  medicine_32820.jpg          (   \n",
              "\n",
              "                                                     bbox  bbox 높이에 대한 너비 비  \\\n",
              "60000   [2063.549032545688, 721.9072080204544, 45.0854...          0.700000   \n",
              "60001   [1878.9132963780062, 713.3194993614924, 163.16...          2.268657   \n",
              "60002   [1976.9988783403421, 792.6044345922198, 266.49...          3.297653   \n",
              "60003   [1542.9191950961197, 860.3840101462162, 326.33...          4.222222   \n",
              "60004   [1584.7842748085595, 937.6733880768736, 322.03...          4.347826   \n",
              "...                                                   ...               ...   \n",
              "102002  [109.49328779974199, 3030.854831940932, 337.68...          2.941489   \n",
              "102003  [1288.428836456075, 906.5898020406504, 276.353...          2.428571   \n",
              "102004  [1039.4533106824476, 1015.1929759969929, 528.3...          4.055687   \n",
              "102005  [1454.0794263845135, 3021.2979179264426, 182.4...          1.565934   \n",
              "102006  [265.68238149494323, 1255.7630390585678, 49.26...          0.423546   \n",
              "\n",
              "        text_length               file_name  \n",
              "60000             1   medicine_32275_32.jpg  \n",
              "60001             4   medicine_32275_33.jpg  \n",
              "60002             7   medicine_32275_34.jpg  \n",
              "60003             9   medicine_32275_35.jpg  \n",
              "60004             7   medicine_32275_36.jpg  \n",
              "...             ...                     ...  \n",
              "102002            4   medicine_32820_97.jpg  \n",
              "102003            3   medicine_32820_98.jpg  \n",
              "102004            9   medicine_32820_99.jpg  \n",
              "102005            2  medicine_32820_100.jpg  \n",
              "102006            1  medicine_32820_101.jpg  \n",
              "\n",
              "[42007 rows x 6 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('data/cropped_image.csv')\n",
        "df_infer = df.iloc[60000:]\n",
        "df_infer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, TrOCRProcessor\n",
        "import torch\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv('data/cropped_image.csv')\n",
        "df_infer = df.iloc[68120:]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained('./model')\n",
        "model.to(device)\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"team-lucid/trocr-small-korean\")\n",
        "# processor = TrOCRProcessor.from_pretrained('./model')\n",
        "# tokenizer=AutoTokenizer.from_pretrained('./model')\n",
        " \n",
        "for idx, data in df_infer.iterrows():\n",
        "    image_folder = 'data/croppedimages'\n",
        "    original_label = data['text']\n",
        "\n",
        "    image = Image.open(os.path.join(image_folder, data['file_name'])).convert('RGB')\n",
        "    pixel_values = processor(image,return_tensors='pt').pixel_values\n",
        "    \n",
        "    generated_ids = model.generate(pixel_values.to(device))\n",
        "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # print('Decoded label = {},{}'.format(original_label,generated_text))\n",
        "\n",
        "    with open('inferenceresult.csv', 'a') as f:\n",
        "        info = [{'file_name': data['file_name'], 'original_label': original_label, 'generated_text':  generated_text }]\n",
        "        writer = csv.DictWriter(f, fieldnames=info[0].keys()) # fieldnames=['Name', 'Height']\n",
        "        # if  data['file_name'] == \"medicine_32275_32.jpg\":\n",
        "        #     writer.writeheader()  # 첫 번째 루프에서만 헤더 쓰기\n",
        "        writer.writerows(info)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42007개 중 정확히 일치하는 데이터 수 : 34739, 다른 데이터 수 : 7268\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "result = pd.read_csv('inferenceresult.csv')\n",
        "result\n",
        "\n",
        "same = 0\n",
        "dif = 0\n",
        "\n",
        "for idx, data in result.iterrows():\n",
        "    if data['original_label'] == data['generated_text'] :\n",
        "        same += 1\n",
        "\n",
        "    else :\n",
        "        dif += 1\n",
        "print(f\"{len(result)}개 중 정확히 일치하는 데이터 수 : {same}, 다른 데이터 수 : {dif}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py39_trocr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
