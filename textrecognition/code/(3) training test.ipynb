{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# small data로 training code 확인해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WcEshvDDW24k"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_img_name</th>\n",
              "      <th>text</th>\n",
              "      <th>bbox</th>\n",
              "      <th>높이에 대한 너비 비</th>\n",
              "      <th>text_length</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>건강</td>\n",
              "      <td>[270.7996407778658, 181.6060866848643, 85.8899...</td>\n",
              "      <td>1.729167</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_31215_0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>기능식품</td>\n",
              "      <td>[231.2671741729349, 237.28789101405005, 163.89...</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>MADE</td>\n",
              "      <td>[2084.202921282695, 176.46746671166335, 243.71...</td>\n",
              "      <td>3.909091</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>IN</td>\n",
              "      <td>[2334.8476798738207, 183.39483441141803, 98.24...</td>\n",
              "      <td>1.677419</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_31215_3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>USA</td>\n",
              "      <td>[2444.426041669941, 185.91387721132884, 150.51...</td>\n",
              "      <td>2.366337</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>Alive!</td>\n",
              "      <td>[741.6678587962095, 333.80412782543954, 1329.4...</td>\n",
              "      <td>1.881068</td>\n",
              "      <td>6</td>\n",
              "      <td>medicine_31215_5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>얼라이브!</td>\n",
              "      <td>[877.3582427556327, 2099.3634109458117, 473.36...</td>\n",
              "      <td>3.402976</td>\n",
              "      <td>5</td>\n",
              "      <td>medicine_31215_6.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>원스데일리</td>\n",
              "      <td>[1388.408311468296, 2103.4166606571503, 554.70...</td>\n",
              "      <td>3.937500</td>\n",
              "      <td>5</td>\n",
              "      <td>medicine_31215_7.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>50+</td>\n",
              "      <td>[1009.8018631112581, 2280.770511880879, 357.22...</td>\n",
              "      <td>2.236220</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_8.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>세트</td>\n",
              "      <td>[1413.5648861099594, 2278.2548544167125, 366.0...</td>\n",
              "      <td>2.049296</td>\n",
              "      <td>2</td>\n",
              "      <td>medicine_31215_9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>OMNI</td>\n",
              "      <td>[979.1715015719071, 2520.454317257905, 306.459...</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_10.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>POTENCY</td>\n",
              "      <td>[1324.3207699275624, 2524.526874996615, 512.12...</td>\n",
              "      <td>5.917647</td>\n",
              "      <td>7</td>\n",
              "      <td>medicine_31215_11.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>건강기능식품</td>\n",
              "      <td>[649.1475607331967, 2743.1406656237054, 495.21...</td>\n",
              "      <td>5.409524</td>\n",
              "      <td>6</td>\n",
              "      <td>medicine_31215_12.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>멀티비타민</td>\n",
              "      <td>[1175.7967553971857, 2712.927413637511, 540.43...</td>\n",
              "      <td>4.135134</td>\n",
              "      <td>5</td>\n",
              "      <td>medicine_31215_13.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>얼라이브</td>\n",
              "      <td>[618.9890696262147, 2907.384853694418, 211.109...</td>\n",
              "      <td>3.646027</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_14.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>원스데일리</td>\n",
              "      <td>[847.6066606553381, 2908.2512820974293, 263.51...</td>\n",
              "      <td>4.454545</td>\n",
              "      <td>5</td>\n",
              "      <td>medicine_31215_15.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>50+</td>\n",
              "      <td>[1130.241581086011, 2911.836503075408, 99.7886...</td>\n",
              "      <td>2.141026</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_16.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>117.5g</td>\n",
              "      <td>[1268.2725887381998, 2913.0315767347342, 175.6...</td>\n",
              "      <td>2.969697</td>\n",
              "      <td>6</td>\n",
              "      <td>medicine_31215_17.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>(979mg</td>\n",
              "      <td>[1461.2769847193993, 2912.434039905071, 216.30...</td>\n",
              "      <td>3.584158</td>\n",
              "      <td>6</td>\n",
              "      <td>medicine_31215_18.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>X</td>\n",
              "      <td>[1695.511421947356, 2914.8241872237236, 37.644...</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_31215_19.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>60정</td>\n",
              "      <td>[1749.887273446703, 2914.2266503940605, 135.04...</td>\n",
              "      <td>2.456522</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_20.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>X</td>\n",
              "      <td>[1901.8673732830816, 2915.1438185072725, 39.79...</td>\n",
              "      <td>0.867347</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_31215_21.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>2개입)</td>\n",
              "      <td>[1956.1769192935656, 2912.3347040584545, 171.3...</td>\n",
              "      <td>2.975610</td>\n",
              "      <td>4</td>\n",
              "      <td>medicine_31215_22.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>R</td>\n",
              "      <td>[2091.1962148155776, 422.6017102260107, 15.574...</td>\n",
              "      <td>0.756410</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_31215_23.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>&amp;</td>\n",
              "      <td>[1731.4303687280014, 2728.508781876943, 80.384...</td>\n",
              "      <td>0.778409</td>\n",
              "      <td>1</td>\n",
              "      <td>medicine_31215_24.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>medicine_31215.jpg</td>\n",
              "      <td>미네랄</td>\n",
              "      <td>[1834.6980184172799, 2715.6003256657837, 323.8...</td>\n",
              "      <td>2.520548</td>\n",
              "      <td>3</td>\n",
              "      <td>medicine_31215_25.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     original_img_name     text  \\\n",
              "0   medicine_31215.jpg       건강   \n",
              "1   medicine_31215.jpg     기능식품   \n",
              "2   medicine_31215.jpg     MADE   \n",
              "3   medicine_31215.jpg       IN   \n",
              "4   medicine_31215.jpg      USA   \n",
              "5   medicine_31215.jpg   Alive!   \n",
              "6   medicine_31215.jpg    얼라이브!   \n",
              "7   medicine_31215.jpg    원스데일리   \n",
              "8   medicine_31215.jpg      50+   \n",
              "9   medicine_31215.jpg       세트   \n",
              "10  medicine_31215.jpg     OMNI   \n",
              "11  medicine_31215.jpg  POTENCY   \n",
              "12  medicine_31215.jpg   건강기능식품   \n",
              "13  medicine_31215.jpg    멀티비타민   \n",
              "14  medicine_31215.jpg     얼라이브   \n",
              "15  medicine_31215.jpg    원스데일리   \n",
              "16  medicine_31215.jpg      50+   \n",
              "17  medicine_31215.jpg   117.5g   \n",
              "18  medicine_31215.jpg   (979mg   \n",
              "19  medicine_31215.jpg        X   \n",
              "20  medicine_31215.jpg      60정   \n",
              "21  medicine_31215.jpg        X   \n",
              "22  medicine_31215.jpg     2개입)   \n",
              "23  medicine_31215.jpg        R   \n",
              "24  medicine_31215.jpg        &   \n",
              "25  medicine_31215.jpg      미네랄   \n",
              "\n",
              "                                                 bbox  높이에 대한 너비 비  \\\n",
              "0   [270.7996407778658, 181.6060866848643, 85.8899...     1.729167   \n",
              "1   [231.2671741729349, 237.28789101405005, 163.89...     3.250000   \n",
              "2   [2084.202921282695, 176.46746671166335, 243.71...     3.909091   \n",
              "3   [2334.8476798738207, 183.39483441141803, 98.24...     1.677419   \n",
              "4   [2444.426041669941, 185.91387721132884, 150.51...     2.366337   \n",
              "5   [741.6678587962095, 333.80412782543954, 1329.4...     1.881068   \n",
              "6   [877.3582427556327, 2099.3634109458117, 473.36...     3.402976   \n",
              "7   [1388.408311468296, 2103.4166606571503, 554.70...     3.937500   \n",
              "8   [1009.8018631112581, 2280.770511880879, 357.22...     2.236220   \n",
              "9   [1413.5648861099594, 2278.2548544167125, 366.0...     2.049296   \n",
              "10  [979.1715015719071, 2520.454317257905, 306.459...     3.500000   \n",
              "11  [1324.3207699275624, 2524.526874996615, 512.12...     5.917647   \n",
              "12  [649.1475607331967, 2743.1406656237054, 495.21...     5.409524   \n",
              "13  [1175.7967553971857, 2712.927413637511, 540.43...     4.135134   \n",
              "14  [618.9890696262147, 2907.384853694418, 211.109...     3.646027   \n",
              "15  [847.6066606553381, 2908.2512820974293, 263.51...     4.454545   \n",
              "16  [1130.241581086011, 2911.836503075408, 99.7886...     2.141026   \n",
              "17  [1268.2725887381998, 2913.0315767347342, 175.6...     2.969697   \n",
              "18  [1461.2769847193993, 2912.434039905071, 216.30...     3.584158   \n",
              "19  [1695.511421947356, 2914.8241872237236, 37.644...     0.807692   \n",
              "20  [1749.887273446703, 2914.2266503940605, 135.04...     2.456522   \n",
              "21  [1901.8673732830816, 2915.1438185072725, 39.79...     0.867347   \n",
              "22  [1956.1769192935656, 2912.3347040584545, 171.3...     2.975610   \n",
              "23  [2091.1962148155776, 422.6017102260107, 15.574...     0.756410   \n",
              "24  [1731.4303687280014, 2728.508781876943, 80.384...     0.778409   \n",
              "25  [1834.6980184172799, 2715.6003256657837, 323.8...     2.520548   \n",
              "\n",
              "    text_length              file_name  \n",
              "0             2   medicine_31215_0.jpg  \n",
              "1             4   medicine_31215_1.jpg  \n",
              "2             4   medicine_31215_2.jpg  \n",
              "3             2   medicine_31215_3.jpg  \n",
              "4             3   medicine_31215_4.jpg  \n",
              "5             6   medicine_31215_5.jpg  \n",
              "6             5   medicine_31215_6.jpg  \n",
              "7             5   medicine_31215_7.jpg  \n",
              "8             3   medicine_31215_8.jpg  \n",
              "9             2   medicine_31215_9.jpg  \n",
              "10            4  medicine_31215_10.jpg  \n",
              "11            7  medicine_31215_11.jpg  \n",
              "12            6  medicine_31215_12.jpg  \n",
              "13            5  medicine_31215_13.jpg  \n",
              "14            4  medicine_31215_14.jpg  \n",
              "15            5  medicine_31215_15.jpg  \n",
              "16            3  medicine_31215_16.jpg  \n",
              "17            6  medicine_31215_17.jpg  \n",
              "18            6  medicine_31215_18.jpg  \n",
              "19            1  medicine_31215_19.jpg  \n",
              "20            3  medicine_31215_20.jpg  \n",
              "21            1  medicine_31215_21.jpg  \n",
              "22            4  medicine_31215_22.jpg  \n",
              "23            1  medicine_31215_23.jpg  \n",
              "24            1  medicine_31215_24.jpg  \n",
              "25            3  medicine_31215_25.jpg  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('trocr/testdata/cropped_image.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e2aWmz_AW24q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "# we reset the indices to start from zero\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fl_UQmfIW24r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, max_target_length=32): # max_target_length 낮출 필요 있음.max(df['text length']) 찍어보기\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text\n",
        "        file_name = self.df['file_name'][idx]\n",
        "        text = self.df['text'][idx]\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(text,\n",
        "                                          padding=\"max_length\",\n",
        "                                          max_length=self.max_target_length).input_ids\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GhZeAkI2YUAe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
        "train_dataset = IAMDataset(root_dir='trocr/testdata/croppedimages/',\n",
        "                           df=train_df,\n",
        "                           processor=processor)\n",
        "eval_dataset = IAMDataset(root_dir='trocr/testdata/croppedimages/',\n",
        "                           df=test_df,\n",
        "                           processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ngHwC15Nagwv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 20\n",
            "Number of validation examples: 6\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xM4CKlaPamFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "encoding = train_dataset[0]\n",
        "for k,v in encoding.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rtsz8KYNaoV-"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=211x58>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aIU5nzVKa8tN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "라이\n"
          ]
        }
      ],
      "source": [
        "labels = encoding['labels']\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qaf6EPb_a_Uc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRcQUmJbFVV"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b_bQks7XbCvN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): DeiTModel(\n",
              "    (embeddings): DeiTEmbeddings(\n",
              "      (patch_embeddings): DeiTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): DeiTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DeiTLayer(\n",
              "          (attention): DeiTAttention(\n",
              "            (attention): DeiTSelfAttention(\n",
              "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): DeiTSelfOutput(\n",
              "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DeiTIntermediate(\n",
              "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DeiTOutput(\n",
              "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): DeiTPooler(\n",
              "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TrOCRForCausalLM(\n",
              "    (model): TrOCRDecoderWrapper(\n",
              "      (decoder): TrOCRDecoder(\n",
              "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
              "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
              "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x TrOCRDecoderLayer(\n",
              "            (self_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (activation_fn): ReLU()\n",
              "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XYkvF3XxbHPj"
      },
      "outputs": [],
      "source": [
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdzekHB9be2l"
      },
      "outputs": [],
      "source": [
        "!pip install datasets jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WzOUi5albaSk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_63639/1679503157.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  cer_metric = load_metric(\"cer\")\n",
            "/home/wooju/anaconda3/envs/py39_trocr/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/cer/cer.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def compute_metrics(pred):\n",
        "#     labels_ids = pred.label_ids\n",
        "#     pred_ids = pred.predictions\n",
        "\n",
        "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "#     labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    \n",
        "#     pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
        "#     label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
        "\n",
        "#     cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "#     return {\"cer\": cer, \"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B4HfDZSBbbpm"
      },
      "outputs": [],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1n9zXxFzcEiR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wooju/anaconda3/envs/py39_trocr/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ce40326bb284574b97b541ed2b2824a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 0: 10.322248458862305\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4f32fee27614a27bdee0f0790a7464d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wooju/anaconda3/envs/py39_trocr/lib/python3.9/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CER: 1.05\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "   # train\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dataloader):\n",
        "    # get the inputs\n",
        "    for k,v in batch.items():\n",
        "      batch[k] = v.to(device)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
        "\n",
        "   # evaluate\n",
        "  model.eval()\n",
        "  valid_cer = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "      # run batch generation\n",
        "      outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
        "      # compute metrics\n",
        "      cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "      valid_cer += cer\n",
        "\n",
        "  print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
        "\n",
        "\n",
        "  with open('result.csv', 'a') as f:\n",
        "    data = [{'epoch': epoch, 'loss': train_loss/len(train_dataloader), 'Validation CER':  valid_cer / len(eval_dataloader) }]\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys()) # fieldnames=['Name', 'Height']\n",
        "    if epoch == 0:\n",
        "        writer.writeheader()  # 첫 번째 루프에서만 헤더 쓰기\n",
        "    writer.writerows(data)\n",
        "\n",
        "  model.save_pretrained(\"./model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOgt-3G4kzhe"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfg-hmv7xZOt"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_json(r'/content/drive/MyDrive/trocr/data/label/TL1/result/medicine/annotations/medicine_00002.json')\n",
        "df_2row = df2.iloc[0]\n",
        "df_2row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnumMVrUuTAV"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i, j in zip(df_2row['annotations']['polygons'], df_2row['annotations'][\"bbox\"]):\n",
        "    print(i['text'], j)\n",
        "\n",
        "for i in  df_2row['annotations'][\"bbox\"]:\n",
        "    print(i['x'],i['x']+i['width'], i['y'],i['y']+i['height'])\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "image1 = Image.open(r'/content/drive/MyDrive/trocr/data/image/TS1/result/medicine/image/medicine_00002.jpg')\n",
        "image1.show()\n",
        "\n",
        "cnt=0\n",
        "\n",
        "import os\n",
        "data = {'file_name' : [], 'text' : []}\n",
        "\n",
        "# 이미지 자르기 crop함수 이용 ex. crop(left,up, rigth, down)\n",
        "for i,j in  zip(df_2row['annotations'][\"bbox\"],df_2row['annotations']['polygons']):\n",
        "    croppedImage=image1.crop((i['x'],i['y'], i['x']+i['width'],i['y']+i['height']))\n",
        "    croppedImage.show()\n",
        "\n",
        "    print(\"잘려진 사진 크기 :\",croppedImage.size)\n",
        "    croppedImage.save('./inferenceresult/croppedImage{}.jpg'.format(cnt))\n",
        "    data['file_name'].append('croppedImage{}.jpg'.format(cnt))\n",
        "    data['text'].append(j['text'])\n",
        "    cnt += 1\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsJ64w8acHW_"
      },
      "outputs": [],
      "source": [
        "# sample_test_df = pd.read_csv('/content/data/test.csv')\n",
        "# sample_test_df.drop(['id'],axis=1,inplace=True)\n",
        "df.file_name=df.file_name.apply(lambda x: './inferenceresult/'+ x )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR86VB3SiS9u"
      },
      "outputs": [],
      "source": [
        "# 저장된 모델 불러오기\n",
        "# from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
        "# model=VisionEncoderDecoderModel.from_pretrained('./model')\n",
        "# tokenizer=AutoTokenizer.from_pretrained('./model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9mTjCbYiWUI"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "img_names, outputs= [], []\n",
        "# device=torch.device('cuda')\n",
        "# model.to(device)\n",
        "\n",
        "for i in tqdm(range(1)):\n",
        "    image = Image.open(df['file_name'][i]).convert('RGB')\n",
        "    # pixel_values =(processor(image,return_tensors='pt').pixel_values).to(device)\n",
        "    pixel_values =processor(image,return_tensors='pt').pixel_values\n",
        "    \n",
        "    generated_ids = model.generate(pixel_values)\n",
        "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    img_names.append(df.file_name[i])\n",
        "    outputs.append(generated_text)\n",
        "    print('Decoded label = {},{}'.format(img_names[i],generated_text))\n",
        "\n",
        "#img_idx = np.random.randint(len(eval_dataaset))\n",
        "#image = Image.open(eval_dataset.dataset_dir + train_df['file_name'][img_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk8G4Bc6iXg_"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'image_names':img_names, 'outputs':outputs})\n",
        "sub=pd.read_csv('/content/data/sample_submission.csv')\n",
        "sub['label']=df.outputs\n",
        "# sub.to_csv('trocr_submit.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py39_trocr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
